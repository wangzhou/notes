qemu操作命令总结
=================

-v0.1 2024.9.27  Sherlock init
-v0.2 2024.9.30  Sherlock add cpu
-v0.3 2024.10.6  Sherlock ...
-v0.4 2024.10.8  Sherlock ...
-v0.5 2024.10.9  Sherlock vNMI/调试/固件/cache/vSVA/SVE/PMU
-v0.6 2024.10.21 Sherlock ...
-v0.7 2024.11.12 Sherlock 增加大页配置

简介：收集qemu各种配置命令。这里一般不区分kvm和tcg，在需要区分的时候，再作说明。
      如果qemu命令和平台相关，则使用ARM64平台。

固件相关
---------

 - DTS

  ARM64 qemu默认使用的是DTS，在不加-bios启动命令的时候就是用DTS启动系统的，qemu
  代码会根据qemu命令行参数生成DTS并放置到内存的特定位置。

 - ACPI

  ARM64 qemu使用-bios可以指定EDK2的UEFI固件，这时qemu使用ACPI的方式启动，和DTS一
  样，qemu代码会自动生成ACPI表格。

  qemu会把各种固件作为submodule加到qemu的仓库里，编译过程生成的各种固件保存在
  qemu/pc-bios/。

  可以用如下的方式直接下载EDK2代码仓编译EDK2固件:
```
  todo: 
```

CPU
----

 - 基本
```
  -smp cpus=8,maxcpus=32,sockets=2,clusters=4,cores=2,threads=2
```
  表示系统最大支持32个逻辑core，启动其中8个，系统中有2个socket，一个socket里有4
  个cluster，一个cluster里有2个物理核，一个物理核有2个逻辑核。

  可见，maxcpus = sockets * clusters * cores * threads。

  如上启动的8个core，会依次分配到第一个socket里的前两个cluster。按照如下冷插的方
  式可以指定启动core的具体位置。

 - 冷插
```
 -device host-arm-cpu,id=core_x,socket-id=x,cluster-id=y,core-id=z,thread-id=n
```
  如上的方式可以指定具体位置的CPU核。

 - 热插拔

  如上的配置中，maxcpus大于系统启动的core数，差值就是支持热插的core的个数，可以
  通过qemu monitor的命令进行CPU热插拔。
```
(qemu) device_add cortex-a57-arm-cpu,id=core2,core-id=2
...
(qemu) device_del core2
```
  如上是tcg模式的CPU热插拔，KVM时把CPU类型改成: host-arm-cpu

  如果qemu启动的时候指定了socket/cluster/core/thread等参数，可以使用如下的方式热
  插CPU，如果对应的位置已经有CPU存在，会报错提示。
```
(qemu) device_add cortex-a57-arm-cpu,id=core_x,socket-id=x,cluster-id=y,core-id=z,thread-id=n
```

 - NUMA
```
  -numa node,memdev=mem0,nodeid=0,cpus=0-15 \
  -numa node,memdev=mem1,nodeid=1,cpus=16-31 \
```

 - cache

  qemu可以根据命令行配置的cache拓扑，生成DTS或ACPI中的对应表格，guest内核根据对应
  的表示就可以知道guest系统的cache拓扑结构。

  目前，qemu中的该特性还处于社区开发阶段。对应的补丁为：
```
  [PATCH v3 0/7] Introduce SMP Cache Topology
  [RFC,0/5] Specifying cache topology on ARM
```

 - 单点特性

   - SVE/SVE2
```
	todo
```

   - PMU
```
	todo
```

内存
-----

 - 基本

  简单配置虚拟机的内存是4096MB。
```
  -m 4096M
```
  
  如下配置是各种内存插拔的基础，如下具体的配置是最大支持内存是4G，启动自带的内存
  是1024MB，还有三个内存插槽可以插内存。
```
  -m 1024M,slots=3,maxmem=4G
```

 - 冷插

  qemu代码里对内存的各种插拔有文档描述，相关文档在qemu/docs/memory-hotplug.txt。
  如下是一个实例的内存冷插的配置，这个配置具有一定的代表性，object表示一个内存后端
  对象，device表示一个内存前端设备，前端设备通过memdev和后端对象关联在一起。
```
qemu [...] -m 6GB,slots=4,maxmem=10G \
  -object memory-backend-file,id=mem1,size=1G,mem-path=/mnt/hugepages-1G \
  -device pc-dimm,id=dimm1,memdev=mem1 \
  -object memory-backend-file,id=mem2,size=256M,mem-path=/mnt/hugepages-2MB \
  -device pc-dimm,id=dimm2,memdev=mem2                                         
```
  如上大页的使用需要作提前的配置，具体方法在Linux内核代码文档里描述，具体路径在：
  Linux/Documentation/admin-guide/mm/hugetlbpage.rst。

  todo: ...
  
 - 热插拔

  在qemu monitor里运行命令进行热插。
```
(qemu) object_add memory-backend-ram,id=mem1,size=1G
(qemu) device_add pc-dimm,id=dimm1,memdev=mem1
```
  对应的热拔命令，device和object的参数是对应的id：
```
(qemu) device_del dimm1
(qemu) object_del mem1
```

 - NUMA
```
  -m 4096M \
  -object memory-backend-ram,id=mem0,size=2048M \
  -object memory-backend-ram,id=mem1,size=2048M \
  -numa node,memdev=mem0,nodeid=0,cpus=0-15 \
  -numa node,memdev=mem1,nodeid=1,cpus=16-31 \
```
  qemu的内存可以和host的特定NUMA绑定，具体配置方式如下：
```
	todo
```

 - nvdimm

  使用nvdimm需要先在-machine里加上nvdimm使能，比如-machine virt,nvdimm=on。qemu
  代码里有nvdimm的使用说明：qemu/docs/nvdimm.txt，简单使用如下：
```
(qemu) object_add memory-backend-ram,id=mem1,size=1G
(qemu) device_add nvdimm,id=nvdimm1,memdev=mem1
```

 - pc-dimm
```
```

 - 使用大页

 qemu可以使用-mem-path指定guest内存使用host上的大页。host可以在内核cmdline或者
 sysfs文件系统指定预留的大页，然后通过mount -f hugetlbfs [大页参数] nonte /mnt/xxx
 挂载对应大页文件系统，随后系统在对应挂载点下mmap分配内存就可以分配到对应的大页。

 qemu -mem-path的参数就是对应大页文件系统的挂载目录。

 mem-path也可以加到如上-object中，参考如上的命令。

 
中断
-----

 ARM64 qemu上，host可以采用GICv3或GICv4的实现，guest上一般都是GICv3。host采用GICv3
 时，虚拟机中断需要kvm显示注入，host采用GICv4以及以上版本时，GIC支持虚拟中断直通。

 - GICv3
```
```

 - GICv4
```
```

 - 伪vNMI/硬件vNMI

IOMMU
------

 - iommufd

```
-object iommufd,id=iommufd0
-device vfio-pci,host=0000:02:00.0,iommufd=iommufd0
```

 - vsmmu

  qemu支持完全用软件模拟的SMMU，可以使用如下命令行使能。
```
qemu-system-aarch64 -machine virt,xxx,iommu=smmuv3
```

 - vSVA

PCIe
-----

 - PCIe root port

 - PCIe switch

 - VFIO直通
```
```

 - 热插拔


网络
-----

 - virtio-net
```
```

存储
-----

 - virtio-blk
```
-device pcie-root-port,port=0x8,chassis=0,id=pci.0,bus=pcie.0,multifunction=on,addr=0x3 \
-device pcie-root-port,port=0x9,chassis=1,id=pci.1,bus=pcie.0,addr=0x3.0x1 \
-device virtio-blk-pci,drive=drive0,id=virtblk0,num-queues=8,packed=on,bus=pci.1 \
-drive file=./boot.img,if=none,id=drive0,format=raw
```
  需要先用dd命令创建如上boot.img文件。

 - virtio-scsi

 - 安装

虚拟机热迁移
-------------

 - 基本热迁移

  热迁移是qemu的基本特性，迁入端迁出端qemu的配置要完全一致。其中迁入端qemu需要增
  加如下命令，表示迁入端的IP和port。(这里是在同一台机器上做迁移，所以IP的地方是0)
```
-incoming tcp:0:6666
```
  迁入端启动后会停下来等待即将到来的迁出端。

  迁出端启动后进入qemu monitor输入迁移命令。
```
migrate -d tcp:0:6666
```
  迁入端在完成迁移后即进入stop状态，在monitor里运行cont命令可以继续运行迁入端虚机。

 - 带VFIO设备热迁移
```
-object iommufd,id=iommufd0
-device vfio-pci,host=0000:02:00.0,iommufd=iommufd0,enable-migration=on
```

 - 热迁移和热插拔联合

```
```

虚拟机和物理机通信
-------------------

 - 9P文件系统
```
-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9,bus=pcie.0 \
-fsdev local,id=p9fs,path=/home/sherlock/p9root,security_model=mapped
```

 - 网络

qemu/kvm调试选项
-----------------

 - qemu启动/停止

 - qemu monitor

  qemu monitor的使用方式收集在[这里](https://wangzhou.github.io/todo)。

  monitor支持通过tcp/udp使用，这个对于使用脚本控制monitor就很友好。 todo

 - telnet/串口/ssh

 - qemu trace

 - kvm trace

 - qemu tcg调试
