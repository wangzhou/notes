qemu riscv timer的基本逻辑

-v0.1 2022.7.26  Sherlock init
-v0.2 2022.8.11  Sherlock 增加如何避免timer误差的逻辑, 是否和真是时间一致
-v0.3 2022.9.15  Sherlock 增加event_handler的初始化逻辑
-v0.4 2022.12.28 Sherlock 增加mtime/mtimecmp寄存器描述，sstc部分描述，U mode timer的描述

简介：本文梳理riscv上timer的基本软硬件逻辑，硬件模型基于qemu，使用的qemu版本是6.2.0，
      内核代码分析使用的版本是v5.12-rc8。

基础逻辑
---------

 riscv上有两个最基本的中断控制器aclint和plic，前者的全称是Advanced Core Local
 Interruptor，是核内的中断控制器，主要是用来产生timer中断和software中断，后者的全称
 是Platform Level Interruptor Controller，主要用来收集外设的中断，plic通过外部中断
 向CPU报中断。

 timer相关的寄存器以及寄存器域段有: mip/mie里和timer相关的域段，mtime以及mtimecmp。
 mie里有控制timer中断使能的bit: MTIE/STIE，控制M mode和S mode timer interrupter是否
 使能，mip里有表示是否存在pending的timer中断的bit: MTIP/STIP。

 mtime是一个可读可写的计数器，其中的数值以一定的时间间隔递增，计数器计满后会回绕，
 mtimecmp寄存器里的数值用来和mtime做比较，当mtime的值大于等于mtimecmp的值，并且
 MTIE使能时，M mode timer中断被触发。

 软件可以在timer中断处理函数里，更新mtimecmp的值，从而维持一个固定周期的时钟中断，
 一般这个中断就是Linux内核的时钟中断。软件可以写STIP触发一个S mode timer中断。

 aclint上把mtime以及mtimecmp抽象成一个M mode timer这样的设备，mtime和mtimecmp是这个
 设备上的MMIO接口，一个M mode timer上有一个mtime，一个M mode timer为服务的每个hart
 设置一个mtimecmp。aclint协议上描述，一个系统可能会有多个M mode timer设备，这样做
 的目的是在CPU存在分层拓扑的时候，比如CPU cluster/node/socket时，一组CPU可以和一个
 M mode timer做在一起，方便这一组CPU的功耗管理。在系统中有多个M mode timer时，需要
 做多个mtime数值上的同步，使得多个mtime之间的误差在一定范围之内。

 alint只定义了M mode timer, riscv的sstc扩展定义了S mode下的timer。

qemu逻辑
---------

 qemu中的aclint和plic的代码路径分别在：hw/intc/riscv_aclint.c和hw/intc/sifive_plic.c。
 这里我们只关注和timer相关的部分，可以看到在riscv_aclint.c里只有M mode timer中断的
 触发代码。

 在qemu上跑内核的时候，发现总是一个M mode timer中断跟着一个S mode timer中断，然后
 再跟一个S mode ecall。qemu里并没有触发S mode timer的代码，可以猜测S mode timer中断
 是在opensbi里触发的。

 整个逻辑是：当mtime大于等于mtimecmp时触发一个M mode中断，opensbi里的中断处理逻辑
 会写STIP，由于S mode time中断已经被委托到S mode处理，在M mode返回S mode后，S mode
 timer中断就会被触发。
```
 /* opensbi/lib/sbi/sbi_trap.c */
 sbi_trap_handler
   +-> sbi_trap_noaia/ais_irq
     +-> sbi_timer_process
           /* 如果没有SSTC特性，才这样处理 */ 
       +-> csr_set(CSR_MIP, MIP_STIP)
```
 S mode timer中断处理函数里通过S mode ecall写mtimecmp，为下一次M mode timer中断配置
 合理的数值。这里面可能有一个问题，mtime如果触发中断后不往前走，就会有时间上的误差，
 可以想象，如果mtime在中断触发后依然往前走，就不会有这个问题。

 查看qemu aclint的代码，其中使用QEMU_CLOCK_VIRTUAL这个时钟来计算mtime寄存器里的值,
 而QEMU_CLOCK_VIRTUAL是来自host上获取时间的函数clock_gettime/get_clock_realtime，
 获取的是一个不断流逝的时间值。

 说到虚拟机的timer，就有一个问题要问，虚拟机里的时间和真实世界里的时间数值上一样么？
 我们看下aclint驱动配置timecmp时，qemu里的timer中断定时是怎么处理里。
```
 riscv_aclint_mtimer_write
   +-> riscv_aclint_mtimer_write_timecmp
     +-> ns_diff = muldiv64(diff, NANOSECONDS_PER_SECOND, timebase_freq)
```
 如上，使用传进来的timecmp值，计算出timer中断间隔的实际值，然后在host上启动一个
 定时器来做模拟，timebase_freq是mtime寄存器作为counter的频率，所以实际时间的计算
 就是：(mtimecmp - 上一个中断点的mtime值) * 1/timebase_freq * NANOSECONDS_PER_SECOND，
 单位是ns，就是上面muldiv64函数的计算结果。所以，虚拟机看到的时间和实际时间是一样的。

Linux内核逻辑
--------------

 内核timer初始化在：arch/riscv/kernel/time.c, time_init
 内核相关驱动的位置在：drivers/clocksource/timer-riscv.c

 如上的timer驱动里会注册timer中断的处理函数riscv_timer_interrupt, 这个函数会进一步
 调用clock_event_device里的event_handler回调函数，但是这个回调函数并不是在驱动里
 直接提供的。

 event_handler的注册逻辑是通过这个驱动里注册的cpu hotplu回调函数riscv_timer_starting_cpu
 完成的，大概的调用逻辑如下：
```
start_kernel
  +-> time_init
    +-> timer_probe
      +-> cpuhp_setup_state
        +-> __cpuhp_setup_state
          +-> __cpuhp_setup_state_cpuslocked
            +-> cpuhp_issue_call
              +-> cpuhp_invoke_callback
                +-> riscv_timer_starting_cpu
                  +-> clockevents_config_and_register
                    +-> clockevents_register_device
                      +-> tick_check_new_device
                        +-> tick_setup_device
                          +-> tick_setup_periodic
			        /* event_handler回调 */
			    +-> tick_handle_periodic
```
 tick_handle_periodic就是每次时钟中断时要运行的逻辑，相关逻辑已经和调度有关系，
 在另外讲调度的文章里独立分析吧。
