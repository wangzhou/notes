Linux内核调度的基本逻辑
------------------------

-v0.1 2022.9.15 Sherlock init
-v0.2 2024.2.23 Sherlock 分析代码
-v0.3 2024.2.26 Sherlock 分析代码
-v0.4 2024.2.27 Sherlock 分析代码
-v0.5 2024.2.28 Sherlock ...

简介：本文分析Linux内核调度的基本逻辑，分析使用的内核版本是5.19-rc8，使用的qemu
      模型是v7.0.0, 硬件构架基于riscv。分析过程重点看下CFS调度。


调度要解决的问题
-----------------

多个线程使用时分复用的方式分享一个CPU core，怎么时分复用。在多核系统下，如何把各
个线程合理的分配到各个核上，这些都是调度要解决的问题。

不同的线程有不同的属性，有的是CPU密集的，有的是IO密集的，有的优先级比较高，调度
器需要尽可能的满足这些需求。

调度的单位有时不只是线程，比如，一个台机器上，还可以限制不同用户对CPU资源的使用
情况，这个也需要调度器参与。

总体上，调度就是根据各种需求，动态的决定CPU资源给谁使用。

调度基本逻辑
-------------

内核里并没有一个固定的点执行全部调度行为，各个执行调度的点散落在内核的各个部分。
调度的逻辑根据系统运行状态进行调度，这些运行状态大概包括：线程的优先级，线程的
运行时间，CPU的负载等等。调度的逻辑抽象出几个调度类: fair_sched_class, rt_sched_class,
dl_sched_class, idle_sched_class等，每个线程和特定的调度类关联，调度类定义的是
具体的调度行为，比如，fair_sched_class(完全公平调度器类)得到的调度行为表现为使用
这种调度的各个线程尽可能公平的使用CPU，而rt_sched_class(实时调度器类)强调的是使用
这种调度器的各个线程不能长时间得不到调度，在规定的时间内，总的都执行下各个线程。

单核上和多核上的调度行为都会依赖负载(load)特点。比如，系统中的线程要尽可能的平均
的分布在系统中的各个核上，首先就需要有一定的手段度量各个核的负载。这里的核可以泛
化到调度组/调度域。调度器根据负载特点，调度各个调度实体占据CPU执行。

线程可以不断的在系统中的各个核上迁移，这里就涉及到迁移的时间点、需要进行迁移条件
以及具体的线程迁移逻辑。

我们先分析调度子系统相关的数据结构。首先应该具有per-cpu的就绪队列和等待队列，就
绪队列里的线程都具有执行条件，但是CPU在一个时间只能运行一个线程，所以把就绪的线
程放到其中排队，等待队列中的线程等待满足运行的条件。线程的主要状态为运行(running)/
可中断睡眠和不可中断睡眠。(可中断睡眠是什么行为?)

调度器在需要调度一个线程运行时就从绪队列中挑选一个线程运行，运行中的线程需要等待
资源的时候，可以把自己放到等待列。调度器只在就绪队列上挑选线程执行，所谓等待队列，
其实是散布在各个子系统里的，当线程需要sleep时，线程把自己加入特定的等待队列，满
足唤醒条件时，其它上下文(一般是对应的中断处理)把等待队列中的线程重新加到就绪队列
参与调度，内核里的wait_for_completion/complete正是一个这样的例子。runnning状态的
线程，使用schedule调度，被换下来的线程还在就绪队列里?

对于如上的调度class，内核按照优先级从高到低的顺序依此调用其中对应的调度回调函数，
rt和dl排在fair之前，但是系统中绝大多数线程属于fair调度。

线程可以主动放弃CPU，从而触发调度，也可以在中断或系统调用返回时触发调度。系统中
还有周期性的时钟中断触发对应的调度行为。一个相关的逻辑是，调度子系统会控制调度行
为，使得在一定的时间内所有线程都得到调度，但是当线程数很多时，强行保证这个逻辑会
使得调度的频率太高，系统做有用功的时间就减少了，所以，当就绪队列上线程太多时，系
统会保证一个线程每次至少运行一段时间再做切换。

问题：

1. 可以把就绪队列中当前没有运行的线程放回等待队列么？
2. 等待队列中的线程，通过什么接口被放到就绪队列？(try_to_wakeup)

代码实现分析
-------------

调度子系统初始化的入口如下：
```
start_kernel
  +-> sched_init
    +-> init_sched_fair_class
```

```
/* 静态定义per-cpu的struct rq数据结构 */
DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues)
```

新线程/进程和调度子系统建立联系逻辑如下：
```
fork系统调用
      /* 创建新进程时，使得新进程和调度子系统关联起来 */
  +-> sched_fork
        /* 根据优先级确定线程的调度类 */
    +-> p->sched_class = &fair_sched_class/&rt_sched_class
```

主动释放CPU触发的调度示例：
```
/* linux/kernel/sched/completion.c */
wait_for_completion
  +-> do_wait_for_common
    +-> schedule_timeout
```
把等待队列里的线程放到就绪队列里:
```
complete
  +-> ...
```

主动调度行为：
```
/* linux/kernel/sched/core.c */
schedule
  +-> __schedule 
    +-> update_rq_clock
          /* 各个时间的概念是什么：rq->clock? */
      +-> update_rq_clock_task
        /*
	 * 从rq里挑出下一个要运行的程序，并更新调度相关的信息。怎么和CFS结合到
	 * 一起?
	 *
	 * rq里的各个域段的含义?
	 */
    +-> next = pick_next_task // __pick_next_task_fair
      +-> do {
            if (curr) { // sched_entity
	      if (curr->on_rq)
		/* 更新vruntime */
	        update_curr(cfs_rq)
	      else
	        curr = NULL
	    }
	    /* 什么意思？*/
	    se = pick_next_entity(cfs_rq)
	    cfs_rq = group_cfs_rq(se)
	  } while (cfs_rq)
    +-> clear_tsk_need_resched(prev)
    +-> clear_preempt_need_resched()
    +-> context_swith(rq, prev, next, &rf)
```
再调用这个函数之前，先把当前线程状态切换到睡眠状态，并把当前线程挂到对应的等待队
列里。

CFS class里的task_fork回调函数，fork的时候会调用到。
```
task_fork_fair
  +-> updata_curr
  +-> place_entity
```

todo: 调度触发的时间点。

todo: 调度相关的系统统计行为。

Note:

sysctl_sched_base_slice，原来的sysctl_sched_min_granularity，每个线程最小运行的
时间。IO线程呢？

normalized_sysctl_sched_base_slice

CONFIG_SMP
CONFIG_NUMA
CONFIG_NUMA_BALANCING
