-v0.1 2024.3.1  sherlock init
-v0.2 2024.3.7  sherlock 分析代码
-v0.3 2024.3.8  sherlock 分析代码
-v0.4 2024.3.11 sherlock 分析代码

简介：本文总结Linux内核里负载均衡的基本逻辑，分析使用的内核版本是6.8-rc5。


基本逻辑
---------

现代处理器系统一般是一个多核系统，每个核上运行的线程数量是不同的，对应每个核上的
工作负载也是不一样。Linux内核的调度子系统可以动态的检测这种不均衡的负载，并动态
的进行负载均衡，所谓负载均衡，就是把高负载的CPU核上的线程迁移到低负载的CPU核上。

Linux内核把整个系统中的CPU核按层级的调度域/调度组做划分，负载均衡逻辑基于调度域/
调度组的划分，调度域/调度组的分析可以参考[这里](https://wangzhou.github.io/Linux中的调度域和调度组/)。

负载均衡逻辑还基于系统对task/CPU核/调度域/调度组的负载的定义，以及各种负载度量指
标的定义。负载均衡的触发逻辑是在time tick中的，时钟中断处理中会进行对应CPU核上的
负载均衡逻辑。负载均衡逻辑从CPU核上的最底层sched_domain起，在该核的各个sched_domain
中做负载均衡，在一个sched_domain里，找见最繁忙的sched_group，然后找见最繁忙sched_group
中最繁忙的CPU核，迁移一定的线程到当前核上。负载均衡逻辑依次在更高层级的sched_domain
做均衡，但是是否进行更高级sched_domain中的负载均衡是看对应的均衡时间是否到来，因
为越高层级的调度域中做负载均衡的开销越大，系统把高层级调度域中的均衡间隔时间配置
的逐渐增大。

可以看到内核里采用一种分布式的方式逐步调整各个核上的负载，目的是使得任务尽可能的
被分配到有余力的CPU核上执行。

我们用如下的示意图说明问题的基本逻辑，其中图中的一个“｜”表示一个线程。假设一开始
core1的就绪队列(rq)上有四个线程需要执行，core0的最底层domain进行负载均衡的时候，
发现本domain里的core1上有排队的线程，这时就把core1上多余的线程迁移到core0上，对应
的状体就是time1到time2的变化。假设core0/core1在一个domain，core2/core3在一个domain，
那么core2的最底层domain进行负载均衡的时候，对于同一个domain里的core2/core3，没有
负载需要均衡，当core2(或core3)的上一级domain进行负载均衡的时候，core0/core1上的
线程被逐渐迁移到core2/core3，这样整个系统集中在core1上的线程被逐渐均衡到core1-core3
上。
```
time1:
                    ||||
           core0   core1   core2   core3
          
time2:
            ||      ||
           core0   core1   core2   core3
          
time3:
             |      ||       |
           core0   core1   core2   core3
          
time4:       |       |       |       |
           core0   core1   core2   core3
```

如上只是一个示意，实际上的负载均衡逻辑由各种各样的情况组成。负载均衡发生在两个
sched_group之间，总是先确定迁移的目标CPU核(所在的sched_group是local group)，然后
找见domain里最繁忙的sched_group，尝试从最繁忙的group上迁移负载到目标核。在这个大
逻辑下，我们可以细化出很多迁移负载的条件，这里面有很多时候是没有必要进行负载迁移
的，其中又有一些和硬件特性相关的边角的迁移逻辑。

当目标CPU核负载相对高的时候，没有必要进行迁移，一般是选local group中的第一个CPU
核或者local group中的idle CPU核作为目标core的。当最繁忙group里还有较多的idle CPU
核的时候，没有必要迁移，因为当均衡发生在繁忙group的idle CPU核上时，group里的负载
自然会在group内部进行迁移。当最繁忙的group的源CPU核上只有一个线程时，也没有必要
迁移，反正迁移也不会带来什么好处。可以想象，还有各种个样的场景需要定义，比如，当
local group和最繁忙group的都超过负载时，如果最繁忙group的负载相对高，就进行负载
均衡，这些具体的场景都不断的加到内核的负载均衡逻辑里，导致现在的负载均衡逻辑代码
相当碎裂...

CPU本身的计算能力也影响负载均衡的策略，比如一个大小核混杂的系统中，就要把重负载
的任务调度到大核上执行。

硬件的SMT特性也会影响负载均衡策略，一个物理核上的多个逻辑核共享一部分硬件资源，
所以相同物理核对应的逻辑核同时执行代码的性能比不同物理核对应的逻辑核同时执行代码
的性能要低，所以，调度的时候需要先把线程安排到空物理核对应的逻辑核上，再把线程安
排到剩余的逻辑核上。例如，把线程分配到core的次序应该是：core0 -> core2 -> core4
-> core1 -> core3。
```
+------------------------------+  +------------------------------+  +-----------------+
| physical core_x              |  | physical core_y              |  | physical core_z |
|                              |  |                              |  |                 |
| logical core0  logical core1 |  | logical core2  logical core3 |  | logical core4   |
+------------------------------+  +------------------------------+  +-----------------+
```

负载均衡还要处理人为线程邦核的情况，邦核的线程显然是不能动了，负载均衡的逻辑就要
绕开被绑的线程，针对其它线程进行调整。

除了时钟中断定时触发负载均衡，系统中的线程在需要调度的时候(wakeup)都会看看哪个CPU
核上更合适执行任务，进而触发线程迁移。系统中可能出现处于不同NUMA的线程共用相同内
存的情况，这样不管怎么做NUMA balancing，始终有垮NUMA使用内存的情况存在，所以NUMA
balancing的逻辑里会检测这种情况，并相应的做线程迁移，把不同NUMA上的线程迁移到相同
NUMA节点上。(todo: nohz场景下的负载均衡)

我们下面先分析调度子系统中各种负载定义，然后分析负载均核的具体逻辑。

各种负载的计算
---------------

线程负载的定义。
CPU核负载的定义。

(侧重怎么进行观测?)

代码分析
---------

在调度子系统基本逻辑分析中，已经知道负载均衡的主要入口点是run_rebalance_domains，
具体逻辑可以参考[这里](https://wangzhou.github.io/Linux内核调度的基本逻辑/)。

负载均衡基本逻辑的代码分析如下：
```
run_rebalance_domains
  +-> nohz_idle_balance
  +-> update_blocked_averages
  +-> rebalance_domains
        /* 遍历每个CPU的各个sched_domain */
    +-> for_each_domain(cpu, sd) {
              /*
               * 得到这一级domain的扫描间隔时间，这个时间可以通过sysfs的接口手动
               * 调整。
               */
          +-> interval = get_sd_balance_interval
              /* 到了时间才进行均衡的逻辑 */
          +-> if (time_after_eq(jiffies, sd->last_balance + interval)) {
                +-> load_balance(cpu, rq, sd, idle, &continue_balancing)
                      /*
                       * 当前core是local group的第一个core，或者local group里有
                       * idle的core。
                       */
                  +-> should_we_balance
                      /*
                       * 其中的逻辑不只是“找见最繁忙的group”，当检测到没有必要作
                       * 均衡时，这个函数直接退出，并返回NULL。
                       */
                  +-> find_busiest_group
                        /*
                         * 遍历domain里各个group，找到最繁忙的那个group，各种
                         * corner的情况也在这里处理。下面把find_busiest_group的
                         * 逻辑展开分析。
                         */
                    +-> update_sd_lb_stats
                      /* 找到最繁忙group里的最繁忙rq */
                  +-> find_busiest_queue
                  +-> detach_tasks
                  +-> attach_tasks
              }
        }
```

这里展开下find_busiest_group的逻辑：
```
find_busiest_group
  +-> init_sd_lb_stats(&sds)
  +-> update_sd_lb_stats(env, &sds)
        /* 遍历domain里的每个group */
    +-> do {
              /* 对于一个group，更新其负载信息 */
          +-> update_sg_lb_stats(env, sds, sg, sgs, &sg_status)
          ^ for_each_cpu_and(i, sched_group_span(group), env->cpus) {
          |       /*
          |        * 每个group的一些统计信息是group中各个core的rq的信息之和：
          |        *
          |        * group_load是group的负载，每个rq平均负载(load_avg)之和。
          |        * group_util?
          |        * group_runnable是每个rq的runnable_avg之和，todo
          |        * sum_nr_running是每个rq h_nr_running之和， todo
          |        */
          |   +-> 更新group_load/group_util/group_runnable/sum_nr_running。
          |       /*
          |        * 这个domain里的core有没有能力不等的，比如大小核，sd flags里
          |        * 的SD_ASYM_CPUCAPACITY这个标记在sd初始化的时候根据固件传入的
          |        * 的信息进行初始化。
          |        */
          |   +-> if (env->sd->flags & SD_ASYM_CPUCAPACITY) {
          |         /*
          |          * 只要有rq里misfit_task_load大于group的group_misfit_task_load，
          |          * 就认为group的计算能力满足不了线程的需要。
          |          *
          |          * misfit_task_load? group_misfit_task_load?
          |          */
          |         if (sgs->group_misfit_task_load < rq->misfit_task_load) {
          |           sgs->group_misfit_task_load = rq->misfit_task_load;
          |           *sg_status |= SG_OVERLOAD;
          |         } if ((env->idle != CPU_NOT_IDLE) && ... ) {
          |           /* todo：不理解这里的逻辑？*/
          |         }
          | }
          |
          | +-> if (!local_group && env->sd->flags & SD_ASYM_PACKING && ...) {
          |       sgs->group_asym_packing = 1
          |     }
          |
          | +-> if (!local_group && smt_balance(env, sgs, group))
          |       sgs->group_smt_balance = 1
          |
          |     /*
          |      * 负载均衡逻辑定义了group的负载类型，根据负载类型进行均衡逻辑，
          |      * 如下group包括local_group。
          |      *
          |      * imbalance_pct是一个可以通过sysfs配置的系数，这个系数大概是117。
          |      */
          | +-> sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs)
          |           /*
          |            * overloaded指group的算力不满足需求了。可见，running线程
          |            * 数量小于核数，不是overloaded； todo
          |            */
          |       +-> group_is_overloaded(imbalance_pct, sgs)
          |           /* group里有用户配置的affinity? */
          |       +-> sg_imbalanced(group)
          |           /* 其它地方配置进来 */
          |       +-> sgs->group_asym_packing/sgs->group_smt_balance/sgs->group_misfit_task_load
          |           /* group_fully_busy指group算力可以满足负载需求 */
          |       +-> group_has_capacity(imbalance_pct, sgs)
          |           /* 剩下情况就是group里的算力有空余了 */
          |
          |     /* todo: ... */
          | +-> sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
          |                     sgs->group_capacity;
          |
          v   /* 如果传入的sg比sds中记录的busier，就更新记录的值*/
          +-> update_sd_pick_busiest(env, sds, sg, sgs)
        }
  +-> /* 不要需要负载均衡时，直接返回NULL。需要作负载均衡，跳到calculate_imbalance */

      /*
       * 对于需要作负载均衡的情况，跳到这里计算不均衡负载的值，同时把group type
       * 转化成migration_type。后续find_busiest_queue根据migration_type作线程迁移
       * 之前的准备。
       *
       * todo: migration_type的语意？
       */
  +-> calculate_imbalance(env, &sds)
```

这里展开下find_busiest_queue的逻辑：
```
find_busiest_queue
  +-> 
```

调试
-----

/proc/loadavg
/proc/stat
/sys/kernel/debug/sched/debug会输出大量调度相关的信息。

1. 各个调度域多长时间均衡一次?
2. 到达怎么样的不平衡程度才要做均衡？
3. 怎么样衡量均衡的效率，就是是否有很多不必要的均衡

