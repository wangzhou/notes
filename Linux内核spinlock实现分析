-v0.1 2023.7.31 Sherlock init
-v0.2 2023.8. 1 Sherlock 增加代码分析

简介：本文简介Linux内核里spinlock实现逻辑，这里会总结下spinlock各种实现的基础逻辑。
      代码分析基于内核v6,5-rc5。知乎上有一个系列的文章已经把这块讲的很好，他的位置
      在这里：https://zhuanlan.zhihu.com/p/100546935

基本逻辑
---------

spinlock的基本行为就是各个CPU core去互斥的处理一个数据，加锁后只有获取锁的core
可以处理数据，解锁后，CPU core又可以去获取锁。整个过程关闭调度。

spinlock最直白的实现方法是多核间用原子的读修改写指令抢一个标记，这个标记原始值
是0，0表示没有core占有这个锁，当一个core原子的检测到这个标记是0，并修改成1时，这个
core占有锁，其它core做这个检测时，这个标记是1，读修改写的原子指令不生效。

这个原子指令大概是这样：CAS(int *val, int old, int new)，如果和old和*val相等，才
把new写入val的地址，把*val的老值保存到new里。

用最直白的逻辑写出的锁实现类似:
```
struct self_spinlock {
	__u32 lock;
};

static inline void self_spinlock(struct self_spinlock *lock)
{
	while (__atomic_test_and_set(&lock->lock, __ATOMIC_ACQUIRE))
		while (__atomic_load_n(&lock->lock, __ATOMIC_RELAXED))
			;
}

static inline void self_unspinlock(struct self_spinlock *lock)
{
	__atomic_clear(&lock->lock, __ATOMIC_RELEASE);
}
```
这样的锁有两个问题：1. 锁的请求顺序和实际获得锁的顺序不一致，因为上面本质上还是
多个core在无序的争抢标记位；2. 多核之间cache会相互影响。
```
   +------+    +------+    +------+    +------+
   | CPU0 |    | CPU1 |    | CPU2 |    | CPU3 |
   +------+    +------+    +------+    +------+
      v           v           v           v
   +------+    +------+    +------+    +------+
   |cache0|    |cache1|    |cache2|    |cache3|
   +------+    +------+    +------+    +------+
            \      \          /     /
             \  +----------------+ /
              \ | Flag in memory |/
                +----------------+
```
对于第二个问题，我们展开看下，在有cache的系统里，系统大概的样子如上，如果CPU0占有
锁，cach0为1(cache0/1/2/3是Flag在各个core上的cache)，CPU1/2/3的cache也会在各个core
读Flag时被设置为1，CPU0释放锁的时候，cache0被写成0，同时CPU1/2/3上对应的cach被无效
化，随后那个core抢先把Flag写成1，对应的cache就是1。后面重复之前的逻辑。可以看出，
本来在unlock core和lock core之间的通行行为被扩展到了所有参与竞争锁的core，不但锁的
请求顺序和实际获得锁的顺序不一致，而且做了很多无用功。

ticket spinlock
----------------

tick spinlock的实现在ARMv6的内核代码里还有保留，具体的路径在linux/arch/arm/include/asm/spinlock.h。
这里只把它核心的逻辑提取出来。

ticket spinlock锁本身的数据结构如下：
```
struct __raw_tickets {
	u16 next;
	u16 owner;
}
```
获取锁的行为就是原子的增加next值，然后作为自己的ticket，拿着自己的ticket一直和owner
做对比，看看是不是轮到自己拿锁，释放锁的行为就是增加owner的值。
```                  
                        +------+    +------+    +------+    +------+
                        | CPU0 |    | CPU1 |    | CPU2 |    | CPU3 |
                        +------+    +------+    +------+    +------+

        lock               ----------------------------------------> 
  local next:              0           1       +---2---+   +---3---+
                                               |       |   |       |
                                     owner++   ^       v   ^       v
      unlock                         ----->    |       |   |       |
owner in __raw_tickets:    0           1       +-owner-+   +-owner-+
```
如上是一个ticket spinlock的示意图，CPU2/3现在在等待锁，CPU1在释放锁。试图获取锁
的CPU原子的对next加1并在本地保存一个加1后的本地next，作为自己的ticket，释放锁的
CPU把锁里的owner值加1，试图获取锁的CPU，一直在拿自己的ticket和owner做比较，如果
ticket和owner相等自己就拿到了锁。

可以看出，ticket spinlock解决了上面的问题1，但是没有解决问题2，因为竞争锁的core
必须时刻拿着自己的ticket和owner做对比，实际上谁得到锁这个信息只要依次传递就好。

MCS spinlock
-------------

基于以上的认识，后来人们又提出了MCS锁，这个名字是用提出这种锁算法的两个人的名字
命名的。

其实从上面ticket spinlock的图上我们已经可以大概看出来要怎么做，就是把每次lock搞
一个锁的副本出来，然后把这些副本用链表链接起来，加锁时还是spin在自己的副本上，解锁
时顺着链表依次释放锁。

大概的示意图如下:
```
                                               ------------+
                                               | lock tail |
                                             / +-----------+
         +------+    +------+    +------+   /+------+
         | CPU0 |    | CPU1 |    | CPU2 |  / | CPU3 |
         +------+    +------+    +------+ /  +------+
                                       1 /        +-----+
                                        /         |     | 3 spin on owner
         +------+    +------+    +------+    +------+   |
         | owner|    | owner|    | owner| 2  | owner|<--+
         | next |--->| next |--->| next |--->| next |
         +------+    +------+    +------+    +------+
```
如上所示，加锁就是找见当前锁链表结尾(步骤1)，把要加的锁节点挂在上面(步骤2)，然后
就spin在自己的owner标记上等待(步骤3)。

解锁就是把当前锁节点的下一个节点的owner配置成1，这样，spin的core检测到owner为1，
就知道现在自己拥有锁了。

MCS锁可以解决上面的两个问题，但是占用的内存比较多了。

qspinlock
----------

spinlock_t的定义在linux/include/linux/spinlock_types.h，封装的数据结结构是struct raw_spinlock，
进一步到arch_spinlock_t, 对于支持qspinlock的构架，arch_spinlock_t的定义就是struct qspinlock。

我们只看小端下的定义，基于此分析lock和unlock的逻辑细节。
```
typedef struct qspinlock {
	union {
		atomic_t val;
		struct {
			u8	locked;
			u8	pending;
		};
		struct {
			u16	locked_pending;
			u16	tail;
		};
	};
} arch_spinlock_t;
```

kernel/locking/qspinlock.c中一个CPU上静态分配一个size是4的struct qnode数组，每个
qnode元素是struct mcs_spinlock的封装。(所有的spinlock实例用这些就够了？)

(tail,          pending, locked)
                locked_pending
<----16bit----><----16bit-----> 
val状态按照如下描述(tail, pending, locked)

qspinlock加锁函数：
```
/* linux/include/asm-generic/qspinlock.h */
static __always_inline void queued_spin_lock(struct qspinlock *lock)
{
	int val = 0;

	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
		return;

	queued_spin_lock_slowpath(lock, val);
}
```
如果val是0，对lock->val原子写入_Q_LOCKED_VAL(0x1)，否则把lock->val的值写入val，
有写入lock->val函数返回true。可见对于没有写入的slowpath拿到lock val当前状态的值，
基于此在slowpath里做处理。如果写入成功val的状态变成(0, 0, 1)。

(这里acquire的语意？)

unlock函数没有用原子写，这里对么?  这里release的语意?
```
static __always_inline void queued_spin_unlock(struct qspinlock *lock)
{
	/*
	 * unlock() needs release semantics:
	 */
	smp_store_release(&lock->locked, 0);
}
```

```
void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
{
	struct mcs_spinlock *prev, *next, *node;
	u32 old, tail;
	int idx;

	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

	/* w: 半虚拟化? */
	if (pv_enabled())
		goto pv_queue;

	/* w: 只有x86支持? */
	if (virt_spin_lock(lock))
		return;

	/*
	 * Wait for in-progress pending->locked hand-overs with a bounded
	 * number of spins so that we guarantee forward progress.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	/* w: 第一个cpu离开，第二个cpu进来之时 */
	if (val == _Q_PENDING_VAL) {
		int cnt = _Q_PENDING_LOOPS;
		val = atomic_cond_read_relaxed(&lock->val,
					       (VAL != _Q_PENDING_VAL) || !cnt--);
	}

	/*
	 * If we observe any contention; queue.
	 */
	/* w: ~_Q_LOCKED_MASK为0xffffff00, 第三个cpu入口 */
	if (val & ~_Q_LOCKED_MASK)
		goto queue;

	/*
	 * trylock || pending
	 *
	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
	 */
	/* w: 返回val的old值，第二个排队cpu入口 */
	val = queued_fetch_set_pending_acquire(lock);

	/*
	 * If we observe contention, there is a concurrent locker.
	 *
	 * Undo and queue; our setting of PENDING might have made the
	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting
	 * on @next to become !NULL.
	 */
	if (unlikely(val & ~_Q_LOCKED_MASK)) {

		/* Undo PENDING if we set it. */
		if (!(val & _Q_PENDING_MASK))
			clear_pending(lock);

		goto queue;
	}

	/*
	 * We're pending, wait for the owner to go away.
	 *
	 * 0,1,1 -> *,1,0
	 *
	 * this wait loop must be a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because not all
	 * clear_pending_set_locked() implementations imply full
	 * barriers.
	 */
	if (val & _Q_LOCKED_MASK)
		smp_cond_load_acquire(&lock->locked, !VAL);

	/*
	 * take ownership and clear the pending bit.
	 *
	 * 0,1,0 -> 0,0,1
	 */
	clear_pending_set_locked(lock);
	lockevent_inc(lock_pending);
	return;

	/*
	 * End of pending bit optimistic spinning and beginning of MCS
	 * queuing.
	 */
queue:
	lockevent_inc(lock_slowpath);
pv_queue:
	node = this_cpu_ptr(&qnodes[0].mcs);
	idx = node->count++;
	tail = encode_tail(smp_processor_id(), idx);

	trace_contention_begin(lock, LCB_F_SPIN);

	/*
	 * 4 nodes are allocated based on the assumption that there will
	 * not be nested NMIs taking spinlocks. That may not be true in
	 * some architectures even though the chance of needing more than
	 * 4 nodes will still be extremely unlikely. When that happens,
	 * we fall back to spinning on the lock directly without using
	 * any MCS node. This is not the most elegant solution, but is
	 * simple enough.
	 */
	if (unlikely(idx >= MAX_NODES)) {
		lockevent_inc(lock_no_node);
		while (!queued_spin_trylock(lock))
			cpu_relax();
		goto release;
	}

	node = grab_mcs_node(node, idx);

	/*
	 * Keep counts of non-zero index values:
	 */
	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);

	/*
	 * Ensure that we increment the head node->count before initialising
	 * the actual node. If the compiler is kind enough to reorder these
	 * stores, then an IRQ could overwrite our assignments.
	 */
	barrier();

	node->locked = 0;
	node->next = NULL;
	pv_init_node(node);

	/*
	 * We touched a (possibly) cold cacheline in the per-cpu queue node;
	 * attempt the trylock once more in the hope someone let go while we
	 * weren't watching.
	 */
	if (queued_spin_trylock(lock))
		goto release;

	/*
	 * Ensure that the initialisation of @node is complete before we
	 * publish the updated tail via xchg_tail() and potentially link
	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
	 */
	smp_wmb();

	/*
	 * Publish the updated tail.
	 * We have already touched the queueing cacheline; don't bother with
	 * pending stuff.
	 *
	 * p,*,* -> n,*,*
	 */
	old = xchg_tail(lock, tail);
	next = NULL;

	/*
	 * if there was a previous node; link it and wait until reaching the
	 * head of the waitqueue.
	 */
	if (old & _Q_TAIL_MASK) {
		prev = decode_tail(old);

		/* Link @node into the waitqueue. */
		WRITE_ONCE(prev->next, node);

		pv_wait_node(node, prev);
		arch_mcs_spin_lock_contended(&node->locked);

		/*
		 * While waiting for the MCS lock, the next pointer may have
		 * been set by another lock waiter. We optimistically load
		 * the next pointer & prefetch the cacheline for writing
		 * to reduce latency in the upcoming MCS unlock operation.
		 */
		next = READ_ONCE(node->next);
		if (next)
			prefetchw(next);
	}

	/*
	 * we're at the head of the waitqueue, wait for the owner & pending to
	 * go away.
	 *
	 * *,x,y -> *,0,0
	 *
	 * this wait loop must use a load-acquire such that we match the
	 * store-release that clears the locked bit and create lock
	 * sequentiality; this is because the set_locked() function below
	 * does not imply a full barrier.
	 *
	 * The PV pv_wait_head_or_lock function, if active, will acquire
	 * the lock and return a non-zero value. So we have to skip the
	 * atomic_cond_read_acquire() call. As the next PV queue head hasn't
	 * been designated yet, there is no way for the locked value to become
	 * _Q_SLOW_VAL. So both the set_locked() and the
	 * atomic_cmpxchg_relaxed() calls will be safe.
	 *
	 * If PV isn't active, 0 will be returned instead.
	 *
	 */
	if ((val = pv_wait_head_or_lock(lock, node)))
		goto locked;

	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));

locked:
	/*
	 * claim the lock:
	 *
	 * n,0,0 -> 0,0,1 : lock, uncontended
	 * *,*,0 -> *,*,1 : lock, contended
	 *
	 * If the queue head is the only one in the queue (lock value == tail)
	 * and nobody is pending, clear the tail code and grab the lock.
	 * Otherwise, we only need to grab the lock.
	 */

	/*
	 * In the PV case we might already have _Q_LOCKED_VAL set, because
	 * of lock stealing; therefore we must also allow:
	 *
	 * n,0,1 -> 0,0,1
	 *
	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
	 *       above wait condition, therefore any concurrent setting of
	 *       PENDING will make the uncontended transition fail.
	 */
	if ((val & _Q_TAIL_MASK) == tail) {
		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
			goto release; /* No contention */
	}

	/*
	 * Either somebody is queued behind us or _Q_PENDING_VAL got set
	 * which will then detect the remaining tail and queue behind us
	 * ensuring we'll see a @next.
	 */
	set_locked(lock);

	/*
	 * contended path; wait for next if not observed yet, release.
	 */
	if (!next)
		next = smp_cond_load_relaxed(&node->next, (VAL));

	arch_mcs_spin_unlock_contended(&next->locked);
	pv_kick_node(lock, next);

release:
	trace_contention_end(lock, 0);

	/*
	 * release the node
	 */
	__this_cpu_dec(qnodes[0].mcs.count);
}
EXPORT_SYMBOL(queued_spin_lock_slowpath);
```
